{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAxczUGVN-Jc"
      },
      "source": [
        "#### CSCE 670 :: Information Storage & Retrieval :: Texas A&M University :: Spring 2025\n",
        "\n",
        "\n",
        "# Homework 1:  OASIS Search Engine, The Beginning\n",
        "\n",
        "### 100 points [4% of your final grade]\n",
        "\n",
        "### Due: February 5 (Wednesday) by 11:59pm\n",
        "\n",
        "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine + explore some LLM capabilities.\n",
        "\n",
        "*Submission instructions (Canvas):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via Canvas (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
        "\n",
        "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
        "\n",
        "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Canvas, search StackOverflow, even use ChatGPT. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. See the course syllabus for details.\n",
        "\n",
        "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
        "\n",
        "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vh7bQ6kCDpr",
        "outputId": "7cc9d7a7-11ae-4220-f0b9-4f859c88ca37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CKllFl-N-Jk"
      },
      "source": [
        "# Dataset: Enron Email Dataset\n",
        "\n",
        "We are providing you with a small collection of emails from the Enron Email Dataset. The Enron Email Dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron. The full corpus contains a total of about 0.5M messages (https://www.cs.cmu.edu/~enron/). For this homework, we will use a small subset of the data. The subset contains 814 emails extracted from the `_sent_mail` of Arnold-J. We have zipped the 814 files (each file contains the information of an email). The zipped file is available on Canvas as `enron_814.zip`. The subset we provide is about 1.1MB. You should treat each email as a unique document to be indexed by your system. You can download the data from Canvas to your local filesystem. We're going to use these emails as the basis of OASIS, our Open Access Searchable Information System!\n",
        "\n",
        "\n",
        "Below is an example of one email.\n",
        "\n",
        "```text\n",
        "Message-ID: <33025919.1075857594206.JavaMail.evans@thyme>\n",
        "Date: Wed, 13 Dec 2000 13:09:00 -0800 (PST)\n",
        "From: john.arnold@enron.com\n",
        "To: slafontaine@globalp.com\n",
        "Subject: re:spreads\n",
        "Mime-Version: 1.0\n",
        "Content-Type: text/plain; charset=us-ascii\n",
        "Content-Transfer-Encoding: 7bit\n",
        "X-From: John Arnold\n",
        "X-To: slafontaine@globalp.com @ ENRON\n",
        "X-cc:\n",
        "X-bcc:\n",
        "X-Folder: \\John_Arnold_Dec2000\\Notes Folders\\'sent mail\n",
        "X-Origin: Arnold-J\n",
        "X-FileName: Jarnold.nsf\n",
        "\n",
        "saw a lot of the bulls sell summer against length in front to mitigate\n",
        "margins/absolute position limits/var.  as these guys are taking off the\n",
        "front, they are also buying back summer.  el paso large buyer of next winter\n",
        "today taking off spreads.  certainly a reason why the spreads were so strong\n",
        "on the way up and such a piece now.   really the only one left with any risk\n",
        "premium built in is h/j now.   it was trading equivalent of 180 on access,\n",
        "down 40+ from this morning.  certainly if we are entering a period of bearish\n",
        "to neutral trade, h/j will get whacked.  certainly understand the arguments\n",
        "for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was\n",
        "trading for 55 on monday.  today it was 10/17.  the market's view of\n",
        "probability of h going crazy has certainly changed in past 48 hours and that\n",
        "has to be reflected in h/j.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "slafontaine@globalp.com on 12/13/2000 04:15:51 PM\n",
        "To: slafontaine@globalp.com\n",
        "cc: John.Arnold@enron.com\n",
        "Subject: re:spreads\n",
        "\n",
        "\n",
        "\n",
        "mkt getting a little more bearish the back of winter i think-if we get another\n",
        "cold blast jan/feb mite move out. with oil moving down and march closer flat\n",
        "px\n",
        "wide to jan im not so bearish these sprds now-less bullish march april as\n",
        "well.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaiOjFANN-Jo"
      },
      "source": [
        "# Part 1: Read and Parse the Email Data (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoiNbqQDrVQ2"
      },
      "source": [
        "Recall how we handled file input in Homework 0? Well, here, our goal is to read the emails so that we can begin to tokenize them later. For this step, you should read the dataset and print the emails. Note that our dataset contains multiple files. You will need to write Python code to read from these files, and then build a list to store the documents. Each item in the list should be a dictionary containing the `Document-ID` as the key, and email content as the value. You can discard all the supplementary information of the email, e.g., `Date`, `From`, `To`, `Subject`, etc.\n",
        "\n",
        "A document should look like:\n",
        "\n",
        "```text\n",
        "{'Document-ID': '33025919.1075857594206',\n",
        "'content': 'saw a lot of the bulls sell summer against length in front to mitigate\n",
        "margins/absolute position limits/var.  as these guys are taking off the\n",
        "front, they are also buying back summer.  el paso large buyer of next winter\n",
        "today taking off spreads.  certainly a reason why the spreads were so strong\n",
        "on the way up and such a piece now.   really the only one left with any risk\n",
        "premium built in is h/j now.   it was trading equivalent of 180 on access,\n",
        "down 40+ from this morning.  certainly if we are entering a period of bearish\n",
        "to neutral trade, h/j will get whacked.  certainly understand the arguments\n",
        "for h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was\n",
        "trading for 55 on monday.  today it was 10/17.  the market's view of\n",
        "probability of h going crazy has certainly changed in past 48 hours and that\n",
        "has to be reflected in h/j.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "slafontaine@globalp.com on 12/13/2000 04:15:51 PM\n",
        "To: slafontaine@globalp.com\n",
        "cc: John.Arnold@enron.com\n",
        "Subject: re:spreads\n",
        "\n",
        "\n",
        "\n",
        "mkt getting a little more bearish the back of winter i think-if we get another\n",
        "cold blast jan/feb mite move out. with oil moving down and march closer flat\n",
        "px\n",
        "wide to jan im not so bearish these sprds now-less bullish march april as\n",
        "well.'\n",
        "}\n",
        "```\n",
        "\n",
        "For this homework, you should treat the email content as a document and the Message-ID as the document ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0sMQsv-mg1l"
      },
      "source": [
        "## Print the first two documents (5 points)\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "DocumentID Document\n",
        "\n",
        "33025919.1075857594206 saw a lot of the bulls sell summer ......\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IfHGvybCysQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dff40d7-dd9a-4835-d7db-f906552eefe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Document-ID': '33025919.1075857594206', 'content': \"saw a lot of the bulls sell summer against length in front to mitigate \\nmargins/absolute position limits/var.  as these guys are taking off the \\nfront, they are also buying back summer.  el paso large buyer of next winter \\ntoday taking off spreads.  certainly a reason why the spreads were so strong \\non the way up and such a piece now.   really the only one left with any risk \\npremium built in is h/j now.   it was trading equivalent of 180 on access, \\ndown 40+ from this morning.  certainly if we are entering a period of bearish \\nto neutral trade, h/j will get whacked.  certainly understand the arguments \\nfor h/j.  if h settles $20, that spread is probably worth $10.  H 20 call was \\ntrading for 55 on monday.  today it was 10/17.  the market's view of \\nprobability of h going crazy has certainly changed in past 48 hours and that \\nhas to be reflected in h/j.\\n\\n\\n\\n\\nslafontaine@globalp.com on 12/13/2000 04:15:51 PM\\nTo: slafontaine@globalp.com\\ncc: John.Arnold@enron.com \\nSubject: re:spreads\\n\\n\\n\\nmkt getting a little more bearish the back of winter i think-if we get another\\ncold blast jan/feb mite move out. with oil moving down and march closer flat \\npx\\nwide to jan im not so bearish these sprds now-less bullish march april as \\nwell.\"}\n",
            "{'Document-ID': '9214363.1075857594228', 'content': 'and they say it was purely coincidental the announcement came today.\\n\\n6 is fine.  \\n\\n\\n\\n\\n\"Jennifer White\" <jenwhite7@zdnetonebox.com> on 12/13/2000 01:18:41 PM\\nTo: John.Arnold@enron.com\\ncc:  \\nSubject: \\n\\n\\nHmmm... interesting news at Enron today.  Should I plan to come to your\\nplace around 6PM?\\n\\n\\n___________________________________________________________________\\nTo get your own FREE ZDNet Onebox - FREE voicemail, email, and fax,\\nall in one place - sign up today at http://www.zdnetonebox.com'}\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# please print out the first 2 docs\n",
        "import os\n",
        "import re\n",
        "\n",
        "def read_emails_from_directory(directory):\n",
        "    emails = []\n",
        "\n",
        "    for i, filename in enumerate(os.listdir(directory)):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                content = file.read()\n",
        "\n",
        "                # Extract Message-ID using regex\n",
        "                message_id_match = re.search(r\"Message-ID:\\s*<([\\d\\.]+)\", content)  # Extract only the numeric part\n",
        "                message_id = message_id_match.group(1).rstrip('.')  # Extract Message-ID\n",
        "\n",
        "                # Remove headers using regex\n",
        "                # After observing the email structure I can see a pattern where the body and header of any email is seperated by 2 newlines i.e \\n\\n\n",
        "                email_body = re.split(r'\\n\\n', content, maxsplit=1)\n",
        "                if len(email_body) > 1:\n",
        "                    email_text = email_body[1]  # Extract content after headers\n",
        "                else:\n",
        "                    email_text = email_body[0]\n",
        "\n",
        "                emails.append({\"Document-ID\": message_id, \"content\": email_text.strip()})\n",
        "\n",
        "    return emails\n",
        "\n",
        "# Example usage\n",
        "dataset_directory = \"/content/drive/MyDrive/ISR/HW1/enron_814\"\n",
        "email_data = read_emails_from_directory(dataset_directory)\n",
        "\n",
        "# Print contents of Documents '1' and '2'\n",
        "for email in email_data:\n",
        "    if email[\"Document-ID\"] in [\"33025919.1075857594206\", \"9214363.1075857594228\"]:\n",
        "      print(email)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-juuk1zLuw"
      },
      "source": [
        "Now that you can read the documents, let's move on to tokenization. We are going to simplify things for you:\n",
        "1. You should **lowercase** all words.\n",
        "2. Replace line breaks (e.g., \\n, \\n\\n), punctuations, dashes and splash (e.g., -, /) and special characters (\\u2019, \\u2005, etc.) with empty space (\" \").\n",
        "3. Tokenize the documents by splitting on whitespace.\n",
        "4. Then only keep words that have a-zA-Z in them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_CpG2ifejjvx"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "import re\n",
        "\n",
        "def preprocess_email_content(email_text):\n",
        "    # Convert to lowercase\n",
        "    email_text = email_text.lower()\n",
        "\n",
        "    # Remove special characters, punctuation, dashes, slashes, etc.\n",
        "    email_text = re.sub(r'[\\n\\r\\t\\-/\\\\\\u2019\\u2005\\u00A0\\u2013\\u2014\\u2026.,!+=*^%$?;:\"\\'(){}\\[\\]<>#@&|~]', ' ', email_text)\n",
        "\n",
        "    # Tokenize by splitting on whitespace\n",
        "    tokens = email_text.split()\n",
        "\n",
        "    # Keep only words containing a-z or A-Z\n",
        "    tokens = [word for word in tokens if re.search(r'[a-zA-Z]', word)]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to extracted emails\n",
        "for email in email_data:\n",
        "    email[\"content\"] = preprocess_email_content(email[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tst9U9DxN-Jp"
      },
      "source": [
        "## Print the first two documents after tokenizing (5 points)\n",
        "\n",
        "Once you have your parser working, you should print the first two documents (documentID and tokens).\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "* DocumentID Tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WcJbBEQgJUd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5246d49-9069-4ab8-9319-a172538cd1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Document-ID': '33025919.1075857594206', 'content': ['saw', 'a', 'lot', 'of', 'the', 'bulls', 'sell', 'summer', 'against', 'length', 'in', 'front', 'to', 'mitigate', 'margins', 'absolute', 'position', 'limits', 'var', 'as', 'these', 'guys', 'are', 'taking', 'off', 'the', 'front', 'they', 'are', 'also', 'buying', 'back', 'summer', 'el', 'paso', 'large', 'buyer', 'of', 'next', 'winter', 'today', 'taking', 'off', 'spreads', 'certainly', 'a', 'reason', 'why', 'the', 'spreads', 'were', 'so', 'strong', 'on', 'the', 'way', 'up', 'and', 'such', 'a', 'piece', 'now', 'really', 'the', 'only', 'one', 'left', 'with', 'any', 'risk', 'premium', 'built', 'in', 'is', 'h', 'j', 'now', 'it', 'was', 'trading', 'equivalent', 'of', 'on', 'access', 'down', 'from', 'this', 'morning', 'certainly', 'if', 'we', 'are', 'entering', 'a', 'period', 'of', 'bearish', 'to', 'neutral', 'trade', 'h', 'j', 'will', 'get', 'whacked', 'certainly', 'understand', 'the', 'arguments', 'for', 'h', 'j', 'if', 'h', 'settles', 'that', 'spread', 'is', 'probably', 'worth', 'h', 'call', 'was', 'trading', 'for', 'on', 'monday', 'today', 'it', 'was', 'the', 'market', 's', 'view', 'of', 'probability', 'of', 'h', 'going', 'crazy', 'has', 'certainly', 'changed', 'in', 'past', 'hours', 'and', 'that', 'has', 'to', 'be', 'reflected', 'in', 'h', 'j', 'slafontaine', 'globalp', 'com', 'on', 'pm', 'to', 'slafontaine', 'globalp', 'com', 'cc', 'john', 'arnold', 'enron', 'com', 'subject', 're', 'spreads', 'mkt', 'getting', 'a', 'little', 'more', 'bearish', 'the', 'back', 'of', 'winter', 'i', 'think', 'if', 'we', 'get', 'another', 'cold', 'blast', 'jan', 'feb', 'mite', 'move', 'out', 'with', 'oil', 'moving', 'down', 'and', 'march', 'closer', 'flat', 'px', 'wide', 'to', 'jan', 'im', 'not', 'so', 'bearish', 'these', 'sprds', 'now', 'less', 'bullish', 'march', 'april', 'as', 'well']}\n",
            "{'Document-ID': '9214363.1075857594228', 'content': ['and', 'they', 'say', 'it', 'was', 'purely', 'coincidental', 'the', 'announcement', 'came', 'today', 'is', 'fine', 'jennifer', 'white', 'jenwhite7', 'zdnetonebox', 'com', 'on', 'pm', 'to', 'john', 'arnold', 'enron', 'com', 'cc', 'subject', 'hmmm', 'interesting', 'news', 'at', 'enron', 'today', 'should', 'i', 'plan', 'to', 'come', 'to', 'your', 'place', 'around', '6pm', 'to', 'get', 'your', 'own', 'free', 'zdnet', 'onebox', 'free', 'voicemail', 'email', 'and', 'fax', 'all', 'in', 'one', 'place', 'sign', 'up', 'today', 'at', 'http', 'www', 'zdnetonebox', 'com']}\n"
          ]
        }
      ],
      "source": [
        "# your code and output here\n",
        "\n",
        "# Print processed emails for Document-ID '1' and '2'\n",
        "for email in email_data:\n",
        "    if email[\"Document-ID\"] in [\"33025919.1075857594206\", \"9214363.1075857594228\"]:\n",
        "        print(email)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2yVi76Ij1ys"
      },
      "source": [
        "## Dictionary Size (5 points)\n",
        "\n",
        "Next you should report the size of your dictionary, that is, how many unique tokens among all the documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "53bAA66zkI55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468fb989-50c5-40a1-d8c1-703da314cefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary Size: 9737\n"
          ]
        }
      ],
      "source": [
        "# your code and output here\n",
        "\n",
        "# Create a set to store unique tokens\n",
        "unique_tokens = set()\n",
        "all_tokens = []\n",
        "\n",
        "# Processed emails and stored in email[\"content\"] from previous step. After that add tokens to the set\n",
        "for email in email_data:\n",
        "    tokens = email[\"content\"]\n",
        "    all_tokens.append(tokens)\n",
        "    unique_tokens.update(tokens)\n",
        "\n",
        "# Report dictionary size\n",
        "dictionary_size = len(unique_tokens)\n",
        "print(f\"Dictionary Size: {dictionary_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU928IKKkJGY"
      },
      "source": [
        "## Top-20 Words (5 points)\n",
        "\n",
        "Finally, you should print a list of the top-20 most popular words by counting among all documents.\n",
        "\n",
        "\n",
        "Your output should look like this:\n",
        "\n",
        "* Rank. Token, Count\n",
        "\n",
        "1. awesome, 20\n",
        "2. cool, 15\n",
        "3. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8MS6n6oNklvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6fb100f-55cf-4253-8f65-48af218ef924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-20 Most Frequent Words:\n",
            "\n",
            "1. the, 4111\n",
            "2. to, 3768\n",
            "3. ect, 2600\n",
            "4. enron, 2241\n",
            "5. a, 1879\n",
            "6. and, 1795\n",
            "7. you, 1749\n",
            "8. of, 1716\n",
            "9. i, 1694\n",
            "10. on, 1590\n",
            "11. john, 1505\n",
            "12. in, 1465\n",
            "13. hou, 1345\n",
            "14. com, 1317\n",
            "15. is, 1202\n",
            "16. for, 1165\n",
            "17. arnold, 991\n",
            "18. subject, 936\n",
            "19. s, 867\n",
            "20. it, 865\n"
          ]
        }
      ],
      "source": [
        "# your code and output here\n",
        "\n",
        "# Initialize an empty dictionary to store word frequencies\n",
        "word_counts = {}\n",
        "\n",
        "# Process each email and update word counts manually\n",
        "for token in all_tokens:\n",
        "    for word in token:\n",
        "        if word in word_counts:\n",
        "            word_counts[word] += 1\n",
        "        else:\n",
        "            word_counts[word] = 1\n",
        "\n",
        "# Sort the dictionary by frequency in descending order\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top-20 most frequent words\n",
        "top_20_words = sorted_word_counts[:20]\n",
        "\n",
        "# Print the results in the required format\n",
        "print(\"\\nTop-20 Most Frequent Words:\\n\")\n",
        "for rank, (word, count) in enumerate(top_20_words, start=1):\n",
        "    print(f\"{rank}. {word}, {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcrFD2F-N-Jp"
      },
      "source": [
        "# Part 2: Boolean Retrieval (30 points)\n",
        "\n",
        "In this part you will build an inverted index to support Boolean retrieval. You should use the tokenization strategy from above.\n",
        "\n",
        "We require your index to support AND, OR, NOT queries.\n",
        "\n",
        "Search for the queries below using your index and print out matching documents (for each query, print out 5 matching documents):\n",
        "* buyer\n",
        "* margins AND limits\n",
        "* winter OR summer\n",
        "* buyers AND risk AND NOT crazy\n",
        "* never OR know\n",
        "\n",
        "Recall, that you should apply the exact same pre-processing strategies to the query as we do to the documents.\n",
        "\n",
        "The output should like this:\n",
        "* DocumentID Document\n",
        "\n",
        "To make our life easier, please output the DocumentIDs in lexicographic order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "GS3Tc_kYN-Jp"
      },
      "outputs": [],
      "source": [
        "# build the index here\n",
        "# add cells as needed to organize your code\n",
        "\n",
        "# Step 1: Build the inverted index\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialize inverted index as a dictionary where each token maps to a set of document IDs\n",
        "inverted_index = defaultdict(set)\n",
        "\n",
        "# Populate the inverted index\n",
        "for email in email_data:\n",
        "    doc_id = email[\"Document-ID\"]\n",
        "    tokens = set(email[\"content\"])  # Use a set to avoid duplicate entries in a document\n",
        "    for token in tokens:\n",
        "        inverted_index[token].add(doc_id)\n",
        "\n",
        "# Step 2: Define Boolean Query Functions\n",
        "def boolean_and(set1, set2):\n",
        "    return set1 & set2  # Intersection (AND operation)\n",
        "\n",
        "def boolean_or(set1, set2):\n",
        "    return set1 | set2  # Union (OR operation)\n",
        "\n",
        "def boolean_not(set1, all_docs):\n",
        "    return all_docs - set1  # Complement (NOT operation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Process Queries\n",
        "def process_query(query, inverted_index, all_docs):\n",
        "    # Process a Boolean query with proper precedence handling.\n",
        "    query_tokens = preprocess_email_content(query)  # Tokenize and lowercase query\n",
        "    if not query_tokens:\n",
        "        return set()\n",
        "\n",
        "    stack = []\n",
        "    operators = []\n",
        "\n",
        "    def apply_operator():\n",
        "        # Apply the last operator in the stack on the last two sets in the stack.\n",
        "        if len(stack) < 2 and operators[-1] != \"not\":\n",
        "            return\n",
        "        op = operators.pop()\n",
        "        if op == \"and\":\n",
        "            right = stack.pop()\n",
        "            left = stack.pop()\n",
        "            stack.append(boolean_and(left, right))\n",
        "        elif op == \"or\":\n",
        "            right = stack.pop()\n",
        "            left = stack.pop()\n",
        "            stack.append(boolean_or(left, right))\n",
        "        elif op == \"not\":\n",
        "            right = stack.pop()\n",
        "            stack.append(boolean_not(right, all_docs))\n",
        "\n",
        "    # Process tokens with operator precedence\n",
        "    # NOT has the highest precedence\n",
        "    for token in query_tokens:\n",
        "        if token in {\"and\", \"or\", \"not\"}:\n",
        "            while operators and operators[-1] == \"not\":\n",
        "                apply_operator()\n",
        "            operators.append(token)\n",
        "        else:\n",
        "            stack.append(inverted_index.get(token, set()))\n",
        "\n",
        "    while operators:\n",
        "        apply_operator()\n",
        "\n",
        "    return stack[0] if stack else set()\n",
        "\n",
        "all_docs = {email[\"Document-ID\"] for email in email_data}  # Set of all document IDs"
      ],
      "metadata": {
        "id": "M1mKz_TB0eMC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82YUVPUknOjX"
      },
      "source": [
        "## Running the five queries (4 points each, 20 points in total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "qWH8h5ZkN-Jp"
      },
      "outputs": [],
      "source": [
        "# search for the input using your index and print out ids of matching documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzx6tpmpjBsX"
      },
      "source": [
        "Now show the results for the query: `buyer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gUOUjF18jFTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4940482-7f71-463a-f706-d9900b050904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: buyer\n",
            "Matching Documents:\n",
            "13960264.1075857658698: very useful thx keep me posted caroline abramo enron am to john arnold hou ect ect mike maggi corp enron enron jennifer fraser hou ect ect cc per sekse ny ect ect subject fund views hi all the funds are trying to figure out what the play is for next year major divergence of opinions most everyone we talk to takes a macro view dwight anderson from tudor thinks anything above is a sale from the perspective of shut in industrial demand he believes that between no industrial basic industry type can operate he tracks all the plant closures similar to what elena does in mike robert s group but it seems on a more comprehensive level jen it would be good for fundamentals to track this number do some scenario analysis on it under various economic conditions like recession i will try to find out what his total number for turned back gas is just ammonia is a little more than bcf which does not seem all that meaningful but the total may bring us back into balance for the summer while he s a seller above he d also be a buyer of summer at lower levels he firmly believes in increased production in still has day short based on his relationship with the producing community although i personally think that 1q02 sees little i know that the fundamentals group is tracking these numbers from the producing community and has seen no increase in 3q over 2q and is doubtful for any in 4q over 3q he really believes theories on products being out of whack heat gas crude propane jim pulaski is a bull period he really likes april and may on the storage scenario i agree that we try to build storage in april but with the backwardation out there what is the incentive as well with lower baseloads for the summer deliverability should not be an issue i am torn on this one catequil new fund paul touradji used to be with tiger they do the same analysis as dwight just starting up and one of their mandates is to be long nat gas through vol or outright harvard not really sure of the view yet they have not been active in gas just crude products they like buying cheap vol because they do not have mtm issues they like to look out in the calendar years the other people we are starting relationships with in the new year are moore capital global advisors caxton paloma partners kingdon renaissance program trader just so you know the names as i get to know them better i ll try to fwd on thoughts regularly have a great holiday i ll be here next week caroline\n",
            "17195279.1075857655281: i think this is the biggest chopfest in nat gas history scale up seller scale down buyer reviewing my aga model and assumptions later today i ll see if i have any new inspirations slafontaine globalp com on am to john arnold enron com cc subject re silverman he will be if we cut him off for a week i bet he gets some inspiration have a good weekdn any view here i think short term range stuff med longer term you know what i think sprds front to backs range bear em at bull em at till they go prompt john arnold enron com on am to steve lafontaine globalco globalco cc fax to subject re silverman ok i would not describe him as the hardest working man in the energy business slafontaine globalp com on am to jarnold enron com cc subject silverman i say we make a pact next time silver man calls in sick on a friday after a typical nite out you and i cut him off for the entire next week deal\n",
            "2268604.1075857652949: fyi bo is a big put buyer and fence seller today though he is trying to defend j slafontaine globalp com on pm to john arnold enron com cc subject re power gen agree on view as u cud tell i got a little less bearish for a bit so i delta hedged and day traded to keep from losing a ton let go og the delta after aga number which was exactly on my forecast still implying bcf swing i actually thot it could have been worse so my range pre was basis past cupla weeks aga that said i with you took my lumps having a very good month in petro helped me hold on to natgas shit p l got most of it back now tho trying to be more patient think we hold s for another week or so must be plenty chopped up traders mite mean we lose some momentum for a bit what the hell was going on today between you and bo seems i see this more and more how come he always seems to be fighting you on this stuff if he would go the same way as you and i this mkt would just crap either way im unwinding gonna be making a jump soon thats p c for the moment will keep u informed of course power gen up last week and cal rate hikes the kiss of death for demand in cal this summer i think john arnold enron com on pm to steve lafontaine globalco globalco cc fax to subject re power gen wow what a week so far beauty of a short squeeze early on even some of the biggest bears i know were covering to reestablish when the market lost its upward momentum unfortunately my boat is too big to play that way takes too long to put the size of the position i manage on or off to play that game just had to sit back and take my lumps couldnt have been a more bearish aga in my opinion got one more decent one and then watch out below amazing that we ve had more demand destruction recently the economy is the pound gorilla that is sitting on nat gas and it aint getting up\n",
            "25732708.1075857656969: don t care about the front i think its vulnerable to a good short squeeze like we saw on thruday and friday trade is getting short in here with cash such a piece if weather ever changes which the weather boys are saying it might in weeks the cash players are going to be big buyers don t really want to carry length on the way down waiting for that to happen though backs are crazy stong cal traded as high as everybody a buyer as california trying to buy any fixed price energy they can find slafontaine globalp com on am to john arnold enron com cc subject re daily charts and matrices as hot links that made me laugh good point any strong view on ngas flat px cuz i dont but seems hard so see it rally much with cash such a dog john arnold enron com on am to steve lafontaine globalco globalco cc fax to subject re daily charts and matrices as hot links he sends me his stuff i like him because he s willing to take a stand so many technicians bullshit their way support at but if it breaks that look for if every technician put specific trades on a sheet with entry and exit points and published them every day a lot of people would be unemployed slafontaine globalp com on am to jarnold enron com cc subject daily charts and matrices as hot links you mite already get this if not ill be happy to forward so let me know interesting comments on both gas and crude for the timing of seasonal lows forwarded by steve lafontaine globalco on am soblander carrfut com on am to soblander carrfut com cc bcc steve lafontaine globalco fax to subject daily charts and matrices as hot links the information contained herein is based on sources that we believe to be reliable but we do not represent that it is accurate or complete nothing contained herein should be considered as an offer to sell or a solicitation of an offer to buy any financial instruments discussed herein any opinions expressed herein are solely those of the author as such they may differ in material respects from those of or expressed or published by on behalf of carr futures or its officers directors employees or affiliates carr futures the charts are now available on the web by clicking on the hot link s contained in this email if for any reason you are unable to receive the charts via the web please contact me via email and i will email the charts to you as attachments crude http www carrfut com research energy1 crude12 pdf natural gas http www carrfut com research energy1 ngas12 pdf distillate http www carrfut com research energy1 hoil12 pdf unleaded http www carrfut com research energy1 unlded12 pdf spot natural gas http www carrfut com research energy1 spotngas12 pdf nat gas strip matrix http www carrfut com research energy1 stripmatrix12 pdf nat gas spread matrix http www carrfut com research energy1 spreadmatrixng12 pdf crude and products spread matrix http www carrfut com research energy1 spreadmatrixcl12 pdf carr futures s wacker dr suite chicago il usa tel fax soblander carrfut com http www carrfut com\n",
            "28376645.1075857655238: i think you need to check your aga model bcf d seems awfully high that s saying that had gas been at the aga for the week would have been look at the aga history for last week this week and next week 3rd week of feb 4th week of feb 1st week of mar do you really think the would have been with no demand destruction it wasn t even really cold that would have been the highest aga of the three weeks by bcf our guys are thinking bcf d pira is saying that feb as a whole averages bcf d that s just destruction and does not include new production if any further pira estimates that march destruction will be about half of feb assuming that gets to equilibrium bcf d demand destruction bcf d more production bcf d more gas generation demand start with a b deficit and injection days gets you to flat against last years storage in current price environment i think we are fairly priced fundamentally but with huge customer imbalance for hedging from buy side surprises in the market right now are to the upside in my view course hard to imagine for j just sell vol slafontaine globalp com on am to john arnold enron com cc subject re silverman my next aga number is sllitely more than pira and assumes bcf d y on y s d swing the higher prices now the uglier it ll be later contmaplating buying some puts for lat 2nd q or early 3q for a few pennies so whaen we go above last years stx in may beleive it or not pero been worse chop fest than gas all i know is going to steamboat co skiing next wed sunday with mans refined prod groups next week so taking postion down to the stuff i really like dont have to worry too much about john arnold enron com on am to steve lafontaine globalco globalco cc fax to subject re silverman i think this is the biggest chopfest in nat gas history scale up seller scale down buyer reviewing my aga model and assumptions later today i ll see if i have any new inspirations slafontaine globalp com on am to john arnold enron com cc subject re silverman he will be if we cut him off for a week i bet he gets some inspiration have a good weekdn any view here i think short term range stuff med longer term you know what i think sprds front to backs range bear em at bull em at till they go prompt john arnold enron com on am to steve lafontaine globalco globalco cc fax to subject re silverman ok i would not describe him as the hardest working man in the energy business slafontaine globalp com on am to jarnold enron com cc subject silverman i say we make a pact next time silver man calls in sick on a friday after a typical nite out you and i cut him off for the entire next week deal\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query = \"buyer\"\n",
        "matching_docs = sorted(process_query(query, inverted_index, all_docs))[:5]\n",
        "print(f\"\\nQuery: {query}\\nMatching Documents:\")\n",
        "for doc_id in matching_docs:\n",
        "    email_content = next(email[\"content\"] for email in email_data if email[\"Document-ID\"] == doc_id)\n",
        "    print(f\"{doc_id}: {' '.join(email_content)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbJ6yn3yMQWY"
      },
      "source": [
        "*Now* show the results for the query: `margins AND limits`\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SmF62rQ_MRlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e13f382-c521-4541-cb41-53faec144d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: margins AND limits\n",
            "Matching Documents:\n",
            "33025919.1075857594206: saw a lot of the bulls sell summer against length in front to mitigate margins absolute position limits var as these guys are taking off the front they are also buying back summer el paso large buyer of next winter today taking off spreads certainly a reason why the spreads were so strong on the way up and such a piece now really the only one left with any risk premium built in is h j now it was trading equivalent of on access down from this morning certainly if we are entering a period of bearish to neutral trade h j will get whacked certainly understand the arguments for h j if h settles that spread is probably worth h call was trading for on monday today it was the market s view of probability of h going crazy has certainly changed in past hours and that has to be reflected in h j slafontaine globalp com on pm to slafontaine globalp com cc john arnold enron com subject re spreads mkt getting a little more bearish the back of winter i think if we get another cold blast jan feb mite move out with oil moving down and march closer flat px wide to jan im not so bearish these sprds now less bullish march april as well\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query = \"margins AND limits\"\n",
        "matching_docs = sorted(process_query(query, inverted_index, all_docs))[:5]\n",
        "print(f\"\\nQuery: {query}\\nMatching Documents:\")\n",
        "for doc_id in matching_docs:\n",
        "    email_content = next(email[\"content\"] for email in email_data if email[\"Document-ID\"] == doc_id)\n",
        "    print(f\"{doc_id}: {' '.join(email_content)}\")  # Print only first 30 words for preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KlaH0fMTO5"
      },
      "source": [
        "Now show the results for the query: `winter OR summer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MAFBtPGmMVRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c451b402-ab45-4f66-c27f-11b739ed55b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: winter OR summer\n",
            "Matching Documents:\n",
            "10353423.1075857652669: maybe hydro situation dire in west think water levels are at recent historical lows problem is from gas standpoint west is an island right now every molecle that can go there is so will provide limited support to prices in east hydro in east is actually very healthy would assume your markets are targeting eastern u s so i dont know if hydro problem in west is that relevant sarah mulholland am to john arnold hou ect ect cc subject re us fuel interesting comment from singapore hope things are going well up there forwarded by sarah mulholland hou ect on am hans wong am to sarah mulholland hou ect ect cc niamh clarke lon ect ect stewart peter lon ect ect caroline cronin eu enron enron angela saenz enron enronxgate subject re us fuel i was reading something interesting last week somewhere on states coping with the coming summer the report was on the amount of ice not huge enough from this winter to provide enough water for hydroelectricity during summer farmers were encouraged to cultivate crops that consume less water the first thing i can think of is low sul fueloil as natgas will be well supported thus european lsfo will be arbg to the states just my thought hi low play worth watching\n",
            "10537445.1075857655390: industrial demand the scary thing no question there are some steel mills and auto factories and plastics plants that were on last november that arent coming up now and its not due to gas prices the economy sucks and it will affect ind demand slafontaine globalp com on am to john arnold enron com cc subject re mkts at least a myn dollars need to talk to pira on that excellant point need to do some margin analyses having said all that look at corporate earnings from last year to this year regardless of natgas costs as a feed industrials will be running slower and consumers just now feeling the pinch as rate increases have only just recently gotten approved and passed to the likes of us and people less fortunate than us john arnold enron com on pm to steve lafontaine globalco globalco cc fax to subject re mkts but that s my point the demand destruction roared its ugly head beginning of jan the price level was of course there is a lag let s make up a lag time say one month on dec cash was trading probably a similar lag on the way down cash is today how much lost demand will there be in a month if we re still million dollar question if you can answer that slafontaine globalp com on pm to john arnold enron com cc subject re mkts off the cuff i wud say tho same goes for gas currently bcf day swing y on y too much to buy this level you need to take the view that industrial america and residential and end users will be able to get back on their feet and recocover a lion share of the bcf d demand destruction this assumes as current very little if any dist fuel switching i think the answer is no not unless the economy was jump starter quickly 2nd q is gone so maybe th 4q remember the demand destruction and industrial shit really just started only weeks ago me thinks it will take longer than that to get back into full swing we all trade the y on y gap all things remaining equal ie term px for the summer starting april we ll have a surplus the other way by july barring a greenhouse in the ohio valley good to hear your view pt as always even if it is wrong ha john arnold enron com on pm to steve lafontaine globalco globalco cc fax to subject re mkts good to hear from you after a great f had an okay g held a lot of term length on the risk reward play figured if we got no weather all the customer and generator buying would be my stop it was amazing that for the drop in price in h the strips have really gone nowhere just a big chop fest i here your arguments but think they are way exagerated agree with bcf d more supply call it with lng imports from canada should be negligible now let s assume price for the summer is no switching full liquids extraction methanol and fertilizer running electric generation demand considering problems in west and very low hydro around bcf d greater this year with normal weather means you have to price bcf d out of market don t think does that what level did we start really losing demand last year it was higher than concerned about recession in industrial sector thats occuring right now think gas is fairly valued here dont think we re going to but i think fear of market considering what happened this past year will keep forward curve very well supported through spring we re already into storage economics so the front goes where the forward curve wants to go slafontaine globalp com on pm to jarnold enron com cc subject mkts its been a while hope all is well not a great few weeks for me in ngas not awful just nothing really working for me and as you know got in front of march apr a cupla times no disasters well im bearish i hate to be so after a buck drop but as i said a month ago to you and now pira coming around gas is a disaster for the natgas demand now production up strongly y on y you guys agree on the production side i know youve been bullish the summer think im stll in the minority but here you go we have y on y supplly up bcf demnad loss 5bcf d bcf day y on y swing then i submit as we started to see due huge rate increases r c demandd energy conservation will be even more dramatic this summer which will effect utilty demand power demand ulitmately if pira rite we lost in jan for this factor i say it cud be bigger this summer as ute loads increase power pxes rise and consumers become poorer there will be more demand flexibilty in the summer part in the midcon and north as ac is more of a luxury item than heat i say lower use in residentail utilty power consumption due rationing is another 1bcf d loss put all this together we wud build an addional apr thru oct on top of last years build basis last year temps and todays prices takes us to tcf or so what am i missing my man summer has to go to bucks or lower to restore demand thots as far as that other thing the p c its still alive shud know more soon and ill keep you posted\n",
            "10603457.1075857597605: thank you for sending this if only you sent it a couple hours earlier just kidding t boone must have heard this because he sold everything today contracts john from jennifer fraser pm to john arnold hou ect ect cc subject pira annual seminar preview hey ja i was at pira today and got a preview of their presenation on oct for their client seminars greg shuttlesworth summary they have turned a little bearish becuase they believe that distillates will cap gas get some to graph ho cl ny no and tz6 index and nx3 the picture is very convincing supply is increasing at a faster pace 1bcf d more in q4 and expect incremental bcf d next summer canadian production is increasing deep water gom is increasing lastly shallow water gom is also improving after years of decline demand modest in more efficient gas turbine economic moderation near term yes it looks a little ugly this winter possibility of shocks in the shoulder low storage and early summer heat in apr may call me or write if you call for more details i am also faxing you their fuel substitution slide it looks at up to bcf d being put back into the supply chain thanks jf\n",
            "11971129.1075857654956: theres only one thing i can think of storage field turning around gives cash market completely different feel instead of utilities looking to sell gas everday they look to buy it huge difference in feel of mrket not so much actual gas but completely different economics of how marginal mmbtu gets priced tightening cash market causes cash players to buy futures hence the tendency for a spring rally every year read heffner today even he talks about it slafontaine globalp com on am to john arnold enron com cc subject re contangos vs winter putspds so let me ask you if they dont buy flat px wfrom here with mega cold east weather cash contangos px only cts from lows after huge apr oct buying what would take us to much higher levels ie whats the risk of being short today clueless and confused john arnold enron com on am to steve lafontaine globalco globalco cc fax to subject re contangos vs winter putspds no real bias today positive numbers sell negative numbers buy looking into other stuff slafontaine globalp com on am to john arnold enron com cc subject re contangos vs winter putspds agreewith all im mega bear summer 2nd q but for the time being weather and as u said uncertainy likely to lend itself so little downside until either weather gets warm or injections get big i dont see the flow as you know but i talk to a cupla utitlities and the bias same as you menioned ive neutralized bear book a bit cuz i cant afford to fite this thing with deep pockets tho i scale up sell next weeks take a bet on ish injections in april and in may ie records aug oct yes low risk wasnt substantially more inverted when we were bucks higher low risk but not a great reward oct nov yea wont make much for another few months on that so it range trades but ill cont to bersd it cuz if end summer that strong im always always more bullish the front of winter other thing i wonder is how wide these summer contangos cud get as everyone so bullish futs for the next few weeks at least weather here sucks to day tree almost fell on me driving into work close one sahud be about ft of white stuff when its said and done dunno how long i can stay but doesnt look all that great for me getting out to steamboat manana heres a hypothetical we agree that demand loss y on y somwhere from to today do you guys think that we can see a substantial demand recovery if prices dont retreat my ffeeling is no for at least another days or more thots any thots on flat px today im slitely long vs bearsds\n",
            "12541953.1075857657123: neal referencing apr oct and nov mar on the following volumes i am feel free to call to transact wood neal neal wood usa conoco com on am to john arnold enron com john arnold enron com cc subject apr01 mar02 strip varying monthly volumes john i am interested in purchasing the following apr01 mar02 nymex strip apr us gas swap nymex mmbtu per month may us gas swap nymex jun us gas swap nymex jul us gas swap nymex aug us gas swap nymex sep us gas swap nymex oct us gas swap nymex nov us gas swap nymex dec us gas swap nymex jan us gas swap nymex feb us gas swap nymex mar us gas swap nymex mmbtu total if interested please indicate enron s offer as well as where you re offering the summer and winter strips online at the time thanks in advance neal wood conoco inc\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query = \"winter OR summer\"\n",
        "matching_docs = sorted(process_query(query, inverted_index, all_docs))[:5]\n",
        "print(f\"\\nQuery: {query}\\nMatching Documents:\")\n",
        "for doc_id in matching_docs:\n",
        "    email_content = next(email[\"content\"] for email in email_data if email[\"Document-ID\"] == doc_id)\n",
        "    print(f\"{doc_id}: {' '.join(email_content)}\")  # Print only first 30 words for preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVcjjsBWIFv"
      },
      "source": [
        "Now show the results for the query: `buyers AND risk AND NOT crazy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3e7Ipk2JWIF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022ee1c4-ce60-47c7-ecd0-5ccd996c77ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: buyers AND risk AND NOT crazy\n",
            "Matching Documents:\n",
            "2726985.1075857655016: forwarded by john arnold hou ect on am from ann m schmidt enron on am to ann m schmidt corp enron enron cc bcc john arnold hou ect subject enron mentions utility deregulation square peg round hole the new york times executives considered to head military los angeles times bush leaning toward execs for military the seattle times enron s chief denies role as energy villain critics regard kenneth lay as deregulation opportunist the san francisco chronicle enron boss says he s not to blame for profits in energy crisis associated press newswires the stadium curse some stocks swoon after arena deals the san francisco chronicle money and business financial desk section economic view utility deregulation square peg round hole by joseph kahn the new york times page column c new york times company washington in the forensic pursuit of what caused california s power failure the bush administration the energy industry and many analysts have granted immunity to deregulation robert shapiro a managing director of enron the giant electricity marketer says the california mess should in no way affect deregulation in other states because california didn t really deregulate spencer abraham the new energy secretary said californians simply goofed setting up a dysfunctional system it is the way california deregulated not deregulation itself that should take the blame they say yet some economists argue that california s troubles should inform the debate about whether not just how to deregulate among them is alfred e kahn the cornell university economist who helped oversee the creation of free markets in the rail trucking and airline industries i am worried about the uniqueness of electricity markets mr kahn said he is still studying whether the design flaws in california s market explain the whole problem but he is sounding a note of skepticism i ve always been uncertain about eliminating vertical integration he said referring to the old ways of allowing a single heavily regulated power company to produce transmit and distribute electricity it may be one industry in which it works reasonably well mr kahn s comments might sound a little heretical when this former carter administration official was pushing deregulation it was still a novel and politically risky concept today getting government out of most businesses is part of the washington economic canon moreover few people believe that california the first state to overhaul its electricity sector from top to bottom has proved a good laboratory to satisfy interest groups the markets were designed in an awkward way which soured some deregulation experts on california before the first electron went on the auction block among the quirks the state required utilities to buy nearly all their power on daily spot markets rather than arranging long term contracts that might have allowed them to hedge risk consumer prices were also fixed making it impossible for utilities to pass on higher wholesale costs paul l joskow an expert on electricity markets at the massachusetts institute of technology and a former student of mr kahn s remains hopeful that the kinks can be ironed out in new england and the the middle atlantic states as well in as britain chile and argentina all places that have restructured electricity markets regulators have had to adjust market rules to correct flaws they have found ways to check the tendency of power sellers to exploit infant markets and charge high prices mr joskow said regulators have also had to establish new markets that through price signals encourage power companies to build enough generating capacity so that they have reserves for peak hours during peak hours shortages and price spikes can substantially raise average prices if they can do it in britain chile and argentina then i think we can do it here mr joskow said still he warns that proper regulation requires tough political choices allowing high prices to pass through to consumers is one making sure nimbyism does not prevent the construction of power plants is another the political system must rise to the task mr joskow said or the old way might be the best we can do mr kahn knows a bit about the old way in the mid s he headed the new york public service commission which oversaw electricity and other regulated industries the drawbacks were legendary local utilities had an endemic tendency to overestimate demand to justify new power plants for which consumers paid through steady rate increases nearly everyone assumed that competition would slash prices but though free markets do a better job managing rail phone and airline prices they have yet to match regulators ability to juggle the complexities of electricity mr kahn said regulators tended to apply heavy political pressure on utilities to keep prices as low as possible and profit margins steady but thin the vertical integration of electricity monopolies may have also had advantages mr kahn said engineers coordinated power plants and transmission lines in ideal ways planners who saw the need for new plants helped find a place for them to be built the players all depended on one another he said california has probably not derailed deregulation efforts but it has made people wonder anew whether market forces work for kilowatts as they do for widgets photo alfred e kahn copyright dow jones company inc all rights reserved national desk executives considered to head military from associated press los angeles times home edition a copyright the times mirror company washington three corporate executives are under consideration to lead the air force army and navy administration officials said saturday the three have been interviewed by defense secretary donald h rumsfeld and the white house was expected to announce this week that it will send their names to the senate for confirmation the washington times reported quoting unidentified sources gordon r england who retired recently from general dynamics corp would be nominated as navy secretary james g roche a vice president at northrop grumman corp is the pick to head the air force and the choice to head the army is thomas e white a retired army general and an executive with enron corp white also once worked as an assistant to colin l powell bush s secretary of state copyright dow jones company inc all rights reserved news bush leaning toward execs for military the associated press the seattle times sunday a9 copyright washington three corporate executives are under consideration to lead the air force army and navy administration officials said yesterday the three men have been interviewed by defense secretary donald rumsfeld and the white house was expected to announce next week that it will send their names to the senate for confirmation the washington times reported yesterday quoting unidentified sources but two bush administration sources speaking on condition of anonymity told the associated press that president bush has not made a decision and that the nominations were not a certainty the times said gordon england who retired last week as a vice president at general dynamics would be nominated as navy secretary england was responsible for the company s information systems and international programs the newspaper also said james roche a vice president at northrop grumman was the pick to head the air force roche a retired navy captain worked in the state department during the reagan administration and later was democratic staff director for the senate armed services committee the nominee for army secretary was said to be thomas white a retired army general and an executive with enron a houston based energy company white was executive assistant to secretary of state colin powell when powell was chairman of the joint chiefs of staff copyright dow jones company inc all rights reserved news enron s chief denies role as energy villain critics regard kenneth lay as deregulation opportunist david lazarus chronicle staff writer the san francisco chronicle final a1 copyright kenneth lay is one of the energy pirates accused by california s governor of fleecing consumers as chairman of enron corp the world s largest energy trader lay is arguably the biggest baddest buccaneer of them all but that s not how he wants to be seen and he certainly doesn t like taking knocks from gov gray davis for having contributed to california s energy mess it s very unfair lay said his brown eyes taking on a puppy dog quality he s trying to vilify us but we didn t make the rules in california we had nothing to do with creating the problem he gazed out from his plush 50th floor office houston s downtown skyscrapers jutted like sharp teeth against the overcast sky everyone played by the rules lay said now our reputations are being maligned in a sense he s right the ultimate blame does rest with california policymakers for deregulating the state s electricity market in such a ham fisted way that power giants like enron cleaned up by exploiting loopholes in the system but enron was no innocent bystander during the restructuring process enron and ken lay were one of the major players behind the push for deregulation in california said janee briesemeister senior policy analyst in the austin office of consumers union a lot of what s happening in california was their idea those familiar with the state s deregulation efforts said enron was especially eager to ensure that a newly created power exchange where wholesale power would be bought and sold was separate from the independent system operator which would oversee the electricity grid this fragmented the wholesale market making it harder to monitor said john rozsa an aide to state sen steve peace d el cajon widely regarded as the godfather of california s bungled deregulation measures enron isn t in the business of making markets work rozsa said they re in the business of making a buck in an ironic twist however enron now could play a pivotal role in helping the state remedy past errors and find its energy footing the company has that much clout seeking lay s blessing thus as the governor pushes ahead with a scheme to purchase the transmission lines of california s cash strapped utilities he didn t hesitate to call recently seeking lay s personal blessing for the plan this must have been a sweet moment for the man who just weeks earlier had been castigated by davis in the governor s state of the state speech i told him we couldn t support it lay said a hint of a smile playing across his lips it will lead to an even less efficient transmission grid and longer term it could make things worse why would davis swallow his pride and court favor with enron s big cheese simple davis will need the bush administration s backing to make the power line sale fly and many believe there s no faster way to reach the new president than via the houston office of his leading corporate patron lay and his company have donated more than to bush s various political campaigns in recent years and he placed enron s private jet at bush s disposal during the presidential race so great is lay s influence with the president that some insist he is now serving effectively as shadow energy secretary shaping u s energy policy as he sees fit there s a long history of enron pulling the levers of its political relationships to get what it wants said craig mcdonald director of texans for public justice a watchdog group what ken lay thinks energy policy should be isn t very different from what george bush and dick cheney think it should be another viewpoint lay of course sees things differently at the mere mention of his close rapport with the president his eyes glazed over and he mechanically recited the words he has repeated numerous times in recent months i have known the president and his family for many years lay said i ve been a strong supporter of his i believe in him and i believe in his policies he insisted that reports of his having sway over bush on energy matters are grossly exaggerated still it is striking that bush s quick decision after taking office to limit federal assistance in solving california s energy woes virtually mirrored lay s own thoughts on the situation so too with the administration s hands off approach to resolving the crisis whatever else california s power woes have been very kind to enron s bottom line the company s revenues more than doubled to billion last year they haven t hurt lay either according to company records his pay package more than tripled last year to million lay and other enron officials steadfastly refuse to break out the company s california earnings from other worldwide business activities but lay conceded that enron s profit from california energy deals last year was not inconsequential we benefit from the volatility he said captive marketplace that s putting it mildly it could be said that california s energy mess was tailor made for enron which is almost uniquely positioned to prosper from a captive marketplace in which electricity and natural gas prices are simultaneously soaring skyward to understand why that is one must look closely at enron s complex business model the company is much more than just a middleman in brokering energy deals lay with a doctorate in economics and a background as a federal energy regulator set about completely reinventing enron in after taking over what was then an unexceptional natural gas pipeline operator as he saw it the real action was not in distribution or generation of energy but in transacting lightning fast deals wherever electricity or gas is needed treating energy like a tradable commodity for the first time enron is now the leader in this fast growing field and uses that advantage to consolidate its position as the market maker of choice for energy buyers and sellers throughout the country it also exploits its size and trading sophistication to structure unusually creative deals for example if electricity prices are down but natural gas prices up enron might cut a deal to meet a utility s power needs in return for taking possession of the gas required to run the utility s plants enron could then turn around and sell that gas elsewhere using part of the proceeds to purchase low priced electricity from another provider which it ships back to the original utility we do best in competitive markets lay said these are sustainable markets trading frenzy enron s trading floors buzz all day long with frantic activity as mostly young mostly male employees scan banks of flat panel displays in search of the best deals rock music blares from speakers giving the scene an almost frat party atmosphere the company s trading volume skyrocketed last year with the advent of an internet based bidding system which logged trades valued at billion making enron by far the world s single biggest e commerce entity kevin presto who oversees enron s east coast power trades called up the california electricity market on his computer with a few quick mouse clicks he showed that enron at that moment was buying power in the golden state at per megawatt hour and selling it at some days we re at some days and some days presto said over the steady thump thump of the trading floor s rock n roll soundtrack there s truly a problem out there this is a recurring theme among enron officials california s electricity market is broken and enron would prefer it if things just settled down as lay himself put it the worst thing for us is a dysfunctional marketplace in reality california s dysfunctional marketplace means enron isn t just making piles of money it s seeing profits both coming and going lots of business in california the company s energy services division which handles the complete energy needs of large institutions counts among its clients the university of california and california state school systems oakland s clorox co and even the san francisco giants and pac bell park enron purchases electricity on behalf of these clients from pacific gas and electric co which by law must keep its rates frozen below current market values at the same time enron sells power to pg e at sky high wholesale levels in other words enron is buying back its own electricity from pg e for just a fraction of the price it charges the utility these guys are the pariahs of the power system said nettie hoge executive director of the utility reform network in san francisco why do we need middlemen they don t do anything except mark up the cost to be fair energy marketers such as enron can help stabilize an efficient marketplace by promoting increased competition between buyers and sellers this has proven the case in pennsylvania where enron actively trades among about market participants but in an inefficient market such as california a company like enron can easily exacerbate things by exploiting loopholes in the state s ill conceived regulatory framework sylvester turner a houston lawmaker who serves as vice chairman of the state committee that oversees texas utilities said he can t blame enron and other power companies for pursuing profits in california california set up some bad rules and these companies played by the rules california set up he said at the end of the day they will behave to enhance their bottom lines but as texas proceeds toward deregulation of its own electricity market next year turner said he has learned from california s experience and is taking steps to prevent texas power giants from shaking down local consumers lessons from golden state he has written a bill intended to give the texas public utility commission more authority in cracking down on market abuses the power companies are fighting the legislation as hard as they can not least among turner s worries is that texas will see what california officials believe happened in their state a deliberate withholding of power by leading providers until surging demand had pushed prices higher i have that concern he said i don t necessarily take these companies at their word for his part lay insists that enron has never deliberately manipulated electricity prices i don t know of any of that he said it s so easy to conjure up conspiracy theories as a sign of enron s commitment to solving california s energy troubles lay said he supported davis when the state began negotiating long term power contracts on behalf of utilities so how many contracts has enron signed suddenly the hurt puppyish expression vanished from lay s face and a harder more steely look glinted from his eyes none he said we won t be signing until we re certain about recovering our costs consider this a shot across california s bow photo caption chairman kenneth lay said enron had nothing to do with creating the energy problem copyright dow jones company inc all rights reserved enron boss says he s not to blame for profits in energy crisis associated press newswires copyright the associated press all rights reserved san francisco ap yes his business has profited handsomely from california s energy crisis but enron corp chairman kenneth lay says he shouldn t be a scapegoat in california s energy crisis that hasn t swayed gov gray davis who has skewered energy companies such as houston based enron for selling expensive power to california never again can we allow out of state profiteers to hold californians hostage davis warned in his state of the state address more recently however davis called lay to discuss negotiations as the state looks to buy power transmission lines from troubled utilities i told him we couldn t support it lay told the san francisco chronicle in an interview at his houston office it will lead to an even less efficient transmission grid and longer term it could make things worse lay is not just any private sector energy czar enron corp is the world s largest energy trader and lay is a close friend of president george bush lay and his corporation have donated more than to bush s various political campaigns in recent years and he offered bush use of enron s private jet during the presidential race but lay said it s economics not politics that matter in california s energy crisis and he thinks it unfair that davis has blamed out of state energy brokers for the protracted problems we didn t make the rules in california lay said we had nothing to do with creating the problem the problem many analysts agree began with the state s deregulation of the power industry in enron encouraged deregulation and the state s ensuing power crisis has been lucrative for the corporation enron s stock jumped percent in and its revenues more than doubled to billion lay was compensated accordingly he received nearly million in stock and cash beyond his million salary last year compared with less than million in bonuses in lay refused to say how much enron has made off california s crisis though he conceded the profit was not inconsequential we benefit from the volatility said lay who took over enron in and has helped turn the corporation into a major player in the trading of electricity as a commodity but lay rejected suggestions that enron has manipulated prices upward by insisting california pay dearly for last minute power that has helped keep the lights on in recent months i don t know of any of that he said it s so easy to conjure up conspiracy theories copyright dow jones company inc all rights reserved business net worth the stadium curse some stocks swoon after arena deals kathleen pender the san francisco chronicle final b1 copyright is buying the name of a big league stadium the kiss of death for a company or does it only seem that way from network associates coliseum in oakland to the unfinished cmgi field outside boston the nation is dotted with sports venues named after companies whose stocks have been sacked the super bowl champion baltimore ravens play in a stadium named after psinet whose stock has fallen percent to a share since it bought naming rights the problem names are not all tech the owners of the twa dome in st louis and pro player stadium in miami are looking for new corporate sponsors because their current ones are bankrupt twa is an airline pro player was part of underwear maker fruit of the loom the home of the anaheim angels could be in the market for a new name if edison international parent of electric utility southern california edison runs out of juice of course these companies were not in trouble when they promised to pay tens or hundreds of millions of dollars to have their names plastered on a ballpark or arena in fact many were at their peak which begs the question should investors get worried when a company in which they own stock puts its name up among the floodlights brian pears head of equity trading with wells capital management wonders if companies are susceptible to some weird strain of the sports illustrated curse it seems as if any athlete who is pictured on the cover of si magazine invariably loses his next game or pulls a groin muscle business celebrities suffer from a similar phenomenon amazon com chief executive officer jeff bezos was named time magazine s person of the year just before his company s stock price tanked don hinchey who advises buyers and sellers in naming rights deals doesn t think the curse holds true in stadium and arena deals you can make a case that a company is doing well when it acquires a naming rights sponsorship but you can t necessarily say it corresponds with a peak in its business says hinchey director of creative services for the bonham group in denver tracking naming firms to find out if hinchey is right i tracked the stock market performance of publicly held companies since they bought naming rights to big league sports venues in north america i excluded facilities named after subsidiaries of larger companies including miller park in milwaukee miller brewing is part of philip morris and pac bell park in san francisco pacific bell is owned by sbc communications which is putting its own name on an arena in san antonio i used the announcement date as a starting point because stadium naming deals are after all marketing endeavors the announcement of a deal generates tons of publicity which is considered positive even if the publicity is negative and even if the stadium won t open for several years then i compared each company s stock market performance with the standard poor s index during the same period the bottom line of the companies that bought stadium or arena names are trading at a higher stock price today than when the deals were announced according to data from factset research systems two companies each bought two names and were counted twice but and this is a big but only of them beat the s p during the period since their respective deals were announced so buying a stadium name might not be a curse but it s no guarantee the company will beat the market winners losers the companies that have done best since buying a name come from a wide variety of industries the biggest winner is qualcomm a wireless telecommunications company although its stock is down percent from its peak it s still up percent since it agreed to slap its name on a san diego stadium the next biggest winners include target discount stores ericsson telecom equipment coors beer fleet financial banking pepsi soft drinks and enron energy the biggest losers are twa psinet internet service provider cmgi internet incubator savvis communications telecom services and network associates network security software network associates stock peaked about three months after it bought naming rights to the oakland coliseum in september since then it has suffered a string of setbacks after the securities and exchange commission questioned its accounting practices it restated its financial results for and its ceo resigned in december network associates is paying slightly more than million per year for the coliseum name it can get out of its year deal after five years the company has been paying us says deena mcclain general counsel with the oakland alameda county coliseum authority we haven t had any discussions with them about changing the contract most naming rights contracts have out clauses that allow the parties to extricate themselves if they want can or need to in the event of financial difficulties or if a team moves says hinchey although nobody likes to be associated with a loser stadium owners may benefit if a troubled company cuts out of a deal early that s because stadium name prices have skyrocketed since the mid 1990s when million a year give or take was average in fedex agreed to pay million over years to be named home of the washington redskins in cmgi agreed to pay million over years to have its name on the new home of the new england patriots it s questionable what kind of shape cmgi will be in when the stadium opens next year the gallon hat of naming rights deals says hinchey is in houston where reliant energy will pay million over years to name the astrodome and a new football stadium after itself some customers of reliant s utility subsidiary were outraged when the deal was announced because the company was also raising electricity rates some shareholders also get perturbed when their company spends money on a stadium instead of a new plant or stock dividends but jim grinstead editor of revenues from sports venues says you have to look at the stadium purchase in light of total marketing budget it sounds like big money but frequently it s over to years if you take out things the company might buy anyway like tickets and luxury suites it s small potatoes what a deal is worth the main benefit of a stadium deal is the exposure a company gets when a game is broadcast on tv or radio or mentioned in print this is the biggest bang for your buck in terms of branding says jennifer keavney a network associates vice president who negotiated the stadium deal she says the cost of her deal about million a year won t even buy you a super bowl ad it will buy five commercials on a nationally televised football game maybe the coliseum perched beside interstate also acts like a giant billboard for the company which frequently gets mentioned in traffic reports hinchey says most naming deals also include tickets and luxury boxes on site exposure through signage and kiosks premium nights when the sponsor might offer samples at the park and inclusion in programs tickets and flyers most companies that strike stadium deals want to become a household name because they sell consumer products or services but not always 3com sold nothing but corporate networking gear when it bought the name to candlestick park in san francisco in it was a good move for them says jim grinstead editor of revenues from sports venues they got the employees they were looking for the visibility they were looking for at the time they were a player in a crowded field and they wanted to look like a fun place to work last april 3com extended its original year contract for two more years the biggest risk companies run is that the team that plays in their facility will be a loser companies invest in an entity that can enhance their brand their sales and hospitality efforts certainly that loses its luster if the team is not performing well hinchey says but corporations realize the team s success on the field fluctuates it could be a champion one year next year in the dumps the same can be said about the corporate sponsors which is something stadium owners be they taxpayers or business tycoons must realize when they sell a name photo caption rich gannon of the oakland raiders scored in oakland s network associates coliseum last year frederic larson the chronicle copyright dow jones company inc all rights reserved\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query = \"buyers AND risk AND NOT crazy\"\n",
        "matching_docs = sorted(process_query(query, inverted_index, all_docs))[:5]\n",
        "print(f\"\\nQuery: {query}\\nMatching Documents:\")\n",
        "for doc_id in matching_docs:\n",
        "    email_content = next(email[\"content\"] for email in email_data if email[\"Document-ID\"] == doc_id)\n",
        "    print(f\"{doc_id}: {' '.join(email_content)}\")  # Print only first 30 words for preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ0sd8zrWIbb"
      },
      "source": [
        "Now show the results for the query: `never OR know`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g2BumYswWIbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8607ff-ab15-4c3e-81d6-2e01e1156283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: never OR know\n",
            "Matching Documents:\n",
            "10008095.1075857595829: not me brian hoskins enron communications pm to john arnold hou ect ect cc subject re never mind did you do that brian t hoskins enron broadband services office mobile fax brian_hoskins enron net forwarded by brian hoskins enron communications on pm john cheng enron sent by john cheng enron pm to john cheng na enron enron cc brian hoskins enron communications enron communications john cheng na enron enron enron communications fangming zhu corp enron enron enron communications pablo torres corp enron enron enron communications don adam corp enron enron enron communications marlin gubser hou ect ect mark hall hou ect subject re all sorry for the lengthy thread a service request has been opened with microsoft on the ie5 issues and her application regards jkc john cheng pm sent by john cheng to brian hoskins enron communications enron communications cc john cheng na enron enron enron communications fangming zhu corp enron enron enron communications pablo torres corp enron enron enron communications don adam corp enron enron enron communications marlin gubser hou ect ect mark hall hou ect subject re brian i apologize if i didn t make myself clear over the phone there is a process that we must follow for software upgrades to ensure compatibility with existing applications and supportability for the helpdesk staff even if it is for a small subset of the population furthermore all applications must be packaged by application integration before it gets rolled out to the desktops as you can see it is more than dispatching desktop support to install the application itself have you guys look at other technologies such as netmeeting it is installed with ie5 and provides real time chat i don t mean to be a pain in the neck but i must advise against ie5 at this time regards jkc brian hoskins enron communications pm to john cheng na enron enron cc fangming zhu corp enron enron pablo torres corp enron enron don adam corp enron enron marlin gubser hou ect ect subject john i talked to fangming and unfortunately her program is not compatible with ie let s proceed with the installation of ie this is an extremely important program that all of the traders will be using if you need additional resources i can arrange for this just let me know what you need these are the people we need to set up with the upgraded version thanks brian\n",
            "10216181.1075857658324: i want john to interview with the various desk heads scott hunter phillip tom i think i m going to tell john not to mention the past it s an issue that doesn t need to be made public and as long as lavo and myself are okay with it i don t see the need to get individual approval from everyone thanks for your help john ed mcmichael am to john arnold hou ect ect cc subject re thanks for the inquiry i sincerely appreciate you giving me the heads up as much as it pains me to say yes i think john has proved himself worthy and i am willing to let him interview for the job as we talked about before my only condition is that you guys make sure you are willing to take him if he comes out on top if there is any chance that his past will negatively influence your decision i am not willing to let him interview he handled the last experience with real maturity but i do not want him to have any more reasons to doubt his ability to overcome his past by working hard and proving himself here he is very valuable to me and ena please let me know ed john arnold pm to ed mcmichael hou ect ect cc subject ed i am starting options on eol in about two weeks as we discussed earlier i don t have the appropriate manpower to run this in certain circumstances such as when i m out of the office as such i d like to bring in john griffith for anohter round of interviews for an options trading role with your permission john\n",
            "10337786.1075857649963: for extra credit if the company is worth more under management a rather than more does your answer change eva pao epao mba2002 hbs edu on pm please respond to epao mba2002 hbs edu to john arnold enron com cc subject re try this one will you do all of my homework original message from john arnold enron com mailto john arnold enron com sent friday may am to epao mba2002 hbs edu subject re try this one i ll pay a grand total of eva pao epao mba2002 hbs edu on pm please respond to epao mba2002 hbs edu to jarnold enron com cc subject try this one please read the following problem very carefully and write in a number at the end you should be ready to defend your answer only a number is allowed not an algebraic equation acquiring a company in the following exercise you will represent company a the acquirer which is currently considering acquiring company t the target by means of a tender offer you plan to tender in cash for of company t s shares but are unsure how high a price to offer the main complication is this the value of company t depends directly on the outcome of a major oil exploration project it is currently undertaking indeed the very viability of company t depends on the exploration outcome if the project fails the company under current management will be worth nothing share but if the project succeeds the value of the company under current management could be as high as share all share values between and are considered equally likely by all estimates the company will be worth considerably more in the hands of company a than under current management in fact whatever the ultimate value under current management the company will be worth fifty percent more under the management of a than under company t if the project fails the company is worth share under either management if the exploration project generates a share value under current management the value under company a is share similarly a share value under company t implies a share value under company a and so on the board of directors of company a has asked you to determine the price they should offer for company t s shares this offer must be made now before the outcome of the drilling project is known from all indications company t would be happy to be acquired by company a provided it is at a profitable price moreover company t wishes to avoid at all cost the potential of a takeover bid by any other firm you expect company t to delay a decision on your bid until the results of the project are in then accept or reject your offer before the news of the drilling results reaches the press thus you company a will not know the results of the exploration project when submitting your price offer but company t will know the results when deciding whether or not to accept your offer in addition company t will accept any offer by company a that is greater than the per share value of the company under current management thus if you offer share for instance company t will accept if the value of the company to company t is anything less than as the representative of company a you are deliberating over price offers in the range of share this is tantamount to making no offer at all to share what price offer per share would you tender for company t s stock per share\n",
            "10353423.1075857652669: maybe hydro situation dire in west think water levels are at recent historical lows problem is from gas standpoint west is an island right now every molecle that can go there is so will provide limited support to prices in east hydro in east is actually very healthy would assume your markets are targeting eastern u s so i dont know if hydro problem in west is that relevant sarah mulholland am to john arnold hou ect ect cc subject re us fuel interesting comment from singapore hope things are going well up there forwarded by sarah mulholland hou ect on am hans wong am to sarah mulholland hou ect ect cc niamh clarke lon ect ect stewart peter lon ect ect caroline cronin eu enron enron angela saenz enron enronxgate subject re us fuel i was reading something interesting last week somewhere on states coping with the coming summer the report was on the amount of ice not huge enough from this winter to provide enough water for hydroelectricity during summer farmers were encouraged to cultivate crops that consume less water the first thing i can think of is low sul fueloil as natgas will be well supported thus european lsfo will be arbg to the states just my thought hi low play worth watching\n",
            "10396784.1075857651750: fuck you 239b3989d msn com on am please respond to leehouse211 asia com to 6na10 msn com cc subject i need your phone to help your debt problem h7gmu how would you like to take all of your debt reduce or eliminate the interest pay less per month and pay them off sooner we have helped over people do just that if you are interested we invite you request our free information by provide the following information full name address city state zip code home phone work phone best time to call e mail address estimated debt size all information is kept securely and never provided to any third party sources this request is totally risk free no obligation or costs are incurred to unsubscribe please hit reply and send a message with remove in the subject\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "query = \"never OR know\"\n",
        "matching_docs = sorted(process_query(query, inverted_index, all_docs))[:5]\n",
        "print(f\"\\nQuery: {query}\\nMatching Documents:\")\n",
        "for doc_id in matching_docs:\n",
        "    email_content = next(email[\"content\"] for email in email_data if email[\"Document-ID\"] == doc_id)\n",
        "    print(f\"{doc_id}: {' '.join(email_content)}\")  # Print only first 30 words for preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAhHuIDfN-Jp"
      },
      "source": [
        "## Observations (10 points)\n",
        "Does your boolean search engine find relevant documents for these queries? As in, would our customers be happy if we shipped this retrieval engine? Why or why not?\n",
        "\n",
        "What is the impact of the pre-processing options? Do they impact your search quality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX6k-Vil0GIy"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "Yes, my boolean search engine does find relevant documents for these queries. But the main limitation of this boolean search engine is if a user’s query does not match exactly with indexed terms, the engine may return no results, even if relevant documents exist with slight variations in wording. Additionally, this boolean search engine treats all matching documents equally without prioritizing more relevant document.\n",
        "\n",
        "Customer's happiness is based on the type of customers we will be dealing with. If our customers require precise control over document retrieval; for example legal, medical, compliance industries; they might find the boolean engine useful. However, for general users, a boolean search engine may feel rigid, leading to frustration due to zero-result queries or overwhelming results without relevance ranking.\n",
        "\n",
        "Yes, pre-processing options does impact our search quality significantly. Without proper pre-processing, boolean search is too rigid. For example, in our case we haven't performed stemming and lemmatization which causes boolean search irrelevant as it will retrieve only the documents based on exact query which matches with our token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5JYoNNrlimy"
      },
      "source": [
        "# Part 3: Ranking (35 points)\n",
        "\n",
        "For the third part, you will add ranking to your search system. Given a search query, our goal is to retrieve the top-5 most relevant emails by assigning a score to each document.\n",
        "\n",
        "We will explore two ranking methods, each offering a different approach to scoring and ranking documents:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMlmJ-8mP8X"
      },
      "source": [
        "### Ranking method A: Ranking with vector space model with TF-IDF (15 points)\n",
        "\n",
        "**Cosine:** You should use cosine as your scoring function.\n",
        "\n",
        "**TFIDF:** For the **document vectors**, use the standard TF-IDF scores introduced in class. For the **query vector**, use **simple weights (the raw term frequency)**. For example:\n",
        "* query: never $\\rightarrow$ (1)\n",
        "* query: never know $\\rightarrow$ (1, 1)\n",
        "\n",
        "**Query:**  `trade`\n",
        "\n",
        "**Output:**\n",
        "You should output the top-5 results plus the cosine score of each of these documents.  \n",
        "\n",
        "The output should be like this:\n",
        "\n",
        "Rank Scores DocumentID Document\n",
        "\n",
        "---\n",
        "\n",
        "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3OMOP3bomAM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be254365-20f6-4d22-d19a-c7ddc32acbbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-5 Retrieved Emails:\n",
            "Rank   Score       Document-ID   Document \n",
            "------------------------------------------------------------\n",
            "1      0.3053     15827855.1075857658654 torrey please set me up to trade crude john...\n",
            "2      0.2994     32835197.1075857597302 hey i just want to confirm the trades i have in your book trade i sell x trade i buy...\n",
            "3      0.2284     2752057.1075857658632 torrey can you also approve mike maggi to trade crude as well thanks for your help john...\n",
            "4      0.1855     5340834.1075857658345 greg what is the correct formula you devised for profitability on last trade is mid...\n",
            "5      0.1788     3383202.1075857656796 you fucker that s my trade i was trying to buy nines the last minutes all i got was scraps...\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Compute TF\n",
        "def compute_tf(doc_tokens):\n",
        "    # Compute term frequency (TF) for a document.\n",
        "    tf = defaultdict(float)\n",
        "    total_terms = len(doc_tokens)\n",
        "    for term in doc_tokens:\n",
        "        tf[term] += 1\n",
        "    for term in tf:\n",
        "        tf[term] = math.log(1+tf[term], 2) # logarithmic base is 2\n",
        "    return tf\n",
        "\n",
        "# Step 2: Compute IDF\n",
        "def compute_idf(documents):\n",
        "    # Compute inverse document frequency (IDF) for the entire corpus.\n",
        "    num_docs = len(documents)\n",
        "    idf = defaultdict(float)\n",
        "    doc_freq = defaultdict(int)\n",
        "\n",
        "    for doc in documents:\n",
        "        unique_terms = set(doc[\"content\"])\n",
        "        for term in unique_terms:\n",
        "            doc_freq[term] += 1\n",
        "\n",
        "    for term, freq in doc_freq.items():\n",
        "        idf[term] = math.log(num_docs / (freq), 2) # logarithmic base is 2\n",
        "\n",
        "    return idf\n",
        "\n",
        "# Step 3: Compute TF-IDF for each document\n",
        "def compute_tf_idf(documents, idf):\n",
        "\n",
        "    # Compute TF-IDF vectors for all documents.\n",
        "    tf_idf_vectors = {}\n",
        "    for doc in documents:\n",
        "        doc_id = doc[\"Document-ID\"]\n",
        "        tf = compute_tf(doc[\"content\"])\n",
        "        tf_idf_vectors[doc_id] = {term: tf[term] * idf[term] for term in tf}\n",
        "\n",
        "    return tf_idf_vectors\n",
        "\n",
        "# Step 4: Compute Cosine Similarity\n",
        "def cosine_similarity(doc_vector, query_vector):\n",
        "    # Compute cosine similarity between a document vector and a query vector.\n",
        "    dot_product = sum(doc_vector.get(term, 0) * query_vector.get(term, 0) for term in query_vector)\n",
        "    doc_magnitude = math.sqrt(sum(value**2 for value in doc_vector.values()))\n",
        "    query_magnitude = math.sqrt(sum(value**2 for value in query_vector.values()))\n",
        "\n",
        "    if doc_magnitude == 0 or query_magnitude == 0:\n",
        "        return 0  # Avoid division by zero\n",
        "\n",
        "    return dot_product / (doc_magnitude * query_magnitude)\n",
        "\n",
        "# Step 5: Process Query and Rank Documents\n",
        "def rank_documents(query, tf_idf_vectors, idf, documents):\n",
        "    # Rank documents based on cosine similarity with the query.\n",
        "    query_tokens = preprocess_email_content(query)  # Tokenize and preprocess query\n",
        "    query_tf = compute_tf(query_tokens)  # Compute TF for query\n",
        "    query_vector = {term: query_tf[term] * idf.get(term, 0) for term in query_tf}  # Compute query TF-IDF\n",
        "\n",
        "    # Compute cosine similarity for each document\n",
        "    scores = []\n",
        "    for doc in documents:\n",
        "        doc_id = doc[\"Document-ID\"]\n",
        "        doc_vector = tf_idf_vectors[doc_id]\n",
        "        similarity = cosine_similarity(doc_vector, query_vector)\n",
        "        scores.append((similarity, doc_id, doc[\"content\"]))\n",
        "\n",
        "    # Sort by descending similarity and return top-5 results\n",
        "    top_5_results = sorted(scores, key=lambda x: x[0], reverse=True)[:5]\n",
        "\n",
        "    print(\"\\nTop-5 Retrieved Emails:\")\n",
        "    print(\"Rank   Score       Document-ID   Document \\n\" + \"-\" * 60)\n",
        "    for rank, (score, doc_id, content) in enumerate(top_5_results, start=1):\n",
        "        print(f\"{rank:<6} {score:.4f}     {doc_id:<12} {' '.join(content[:20])}...\")  # Print first 20 words\n",
        "\n",
        "# Step 6: Run the Retrieval System\n",
        "idf_scores = compute_idf(email_data)\n",
        "tf_idf_vectors = compute_tf_idf(email_data, idf_scores)\n",
        "\n",
        "# Example query\n",
        "query = \"trade\"\n",
        "rank_documents(query, tf_idf_vectors, idf_scores, email_data)\n",
        "\n",
        "\n",
        "# print out the top-5 retrieved emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf3-HsRamBkh"
      },
      "source": [
        "### Ranking method B: Ranking with BM25 (15 points)\n",
        "Finally, let's try the BM25 approach for ranking. Refer to https://en.wikipedia.org/wiki/Okapi_BM25 for the specific formula. You could choose k_1 = 1.2 and b = 0.75 but feel free to try other options.\n",
        "\n",
        "**Query:**  `gas floor`\n",
        "\n",
        "**Output:**\n",
        "You should output the top-5 results plus the BM25 score of each of these documents.  \n",
        "\n",
        "The output should be like this:\n",
        "\n",
        "Rank Scores DocumentID Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lESwxZNImIle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2555877-ed49-4f2b-d4f9-3672abacc1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-5 Retrieved Emails (BM25):\n",
            "Rank   Score       Document-ID   Document \n",
            "------------------------------------------------------------\n",
            "1      7.6584     29559946.1075857598198 thx for the spreadsheet questions what time frame does this entail and does the correlation between the trader and agg...\n",
            "2      6.8478     32732331.1075857597410 john i have asked mike and larry to spend half an hour each talking to you about opportunities on the...\n",
            "3      6.8220     23846275.1075857658302 john i would like for you to come talk to a couple more people on the gas floor about a...\n",
            "4      6.4052     8915800.1075857597866 thanks this is exactly what i wanted john enron north america corp from sunil dalal enron pm to john arnold...\n",
            "5      6.0879     3417404.1075857651247 can you send jean a list of her seat numbers forwarded by john arnold hou ect on pm from jean...\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "\n",
        "# BM25 Parameters\n",
        "k1 = 1.2\n",
        "b = 0.75\n",
        "\n",
        "# Step 1: Compute IDF for BM25\n",
        "def compute_bm25_idf(documents):\n",
        "    # Compute Inverse Document Frequency (IDF) for BM25.\n",
        "    num_docs = len(documents)\n",
        "    idf = defaultdict(float)\n",
        "    doc_freq = defaultdict(int)\n",
        "\n",
        "    for doc in documents:\n",
        "        unique_terms = set(doc[\"content\"])  # Unique terms in the document\n",
        "        for term in unique_terms:\n",
        "            doc_freq[term] += 1  # Count documents containing term\n",
        "\n",
        "    for term, freq in doc_freq.items():\n",
        "        idf[term] = math.log((num_docs - freq + 0.5) / (freq + 0.5) + 1)  # BM25 IDF formula # Here the log base is e by default\n",
        "\n",
        "    return idf\n",
        "\n",
        "# Step 2: Compute BM25 Score for a Document\n",
        "def compute_bm25_score(query_tokens, document, idf, avg_doc_len):\n",
        "    # Compute BM25 score for a document given a query.\n",
        "    doc_length = len(document[\"content\"])\n",
        "    tf = defaultdict(int)\n",
        "\n",
        "    for term in document[\"content\"]:\n",
        "        tf[term] += 1  # Count term frequency in the document\n",
        "\n",
        "    score = 0\n",
        "    for term in query_tokens:\n",
        "        if term in tf:\n",
        "            term_freq = tf[term]\n",
        "            numerator = term_freq * (k1 + 1)\n",
        "            denominator = term_freq + k1 * (1 - b + b * (doc_length / avg_doc_len))\n",
        "            score += idf[term] * (numerator / denominator)  # Apply BM25 formula\n",
        "\n",
        "    return score\n",
        "\n",
        "# Step 3: Rank Documents using BM25\n",
        "def rank_documents_bm25(query, documents):\n",
        "    # Rank documents using BM25 given a query.\n",
        "    query_tokens = preprocess_email_content(query)  # Tokenize and preprocess query\n",
        "    idf = compute_bm25_idf(documents)  # Compute IDF for BM25\n",
        "    avg_doc_len = sum(len(doc[\"content\"]) for doc in documents) / len(documents)  # Compute average doc length\n",
        "\n",
        "    # Compute BM25 score for each document\n",
        "    scores = []\n",
        "    for doc in documents:\n",
        "        doc_id = doc[\"Document-ID\"]\n",
        "        score = compute_bm25_score(query_tokens, doc, idf, avg_doc_len)\n",
        "        scores.append((score, doc_id, doc[\"content\"]))\n",
        "\n",
        "    # Sort by descending score and return top-5 results\n",
        "    top_5_results = sorted(scores, key=lambda x: x[0], reverse=True)[:5]\n",
        "\n",
        "    return top_5_results\n",
        "\n",
        "# Step 4: Run BM25 Ranking\n",
        "query = \"gas floor\"\n",
        "top_5_results = rank_documents_bm25(query, email_data)\n",
        "print(\"\\nTop-5 Retrieved Emails (BM25):\")\n",
        "print(\"Rank   Score       Document-ID   Document \\n\" + \"-\" * 60)\n",
        "for rank, (score, doc_id, content) in enumerate(top_5_results, start=1):\n",
        "    print(f\"{rank:<6} {score:.4f}     {doc_id:<12} {' '.join(content[:20])}...\")\n",
        "\n",
        "\n",
        "# print out the top-5 retrieved emails"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison between Cosine similarity function and BM25"
      ],
      "metadata": {
        "id": "pwJIO-FnyCAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieved top-5 emails for query=\"trade\" for comparison\n",
        "query = \"trade\"\n",
        "top_5_results = rank_documents_bm25(query, email_data)\n",
        "print(\"\\nTop-5 Retrieved Emails (BM25):\")\n",
        "print(\"Rank   Score       Document-ID   Document \\n\" + \"-\" * 60)\n",
        "for rank, (score, doc_id, content) in enumerate(top_5_results, start=1):\n",
        "    print(f\"{rank:<6} {score:.4f}     {doc_id:<12} {' '.join(content[:20])}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVQ9bCVIxq3m",
        "outputId": "b459941e-2993-477c-f78b-20435fb61601"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-5 Retrieved Emails (BM25):\n",
            "Rank   Score       Document-ID   Document \n",
            "------------------------------------------------------------\n",
            "1      4.5414     32835197.1075857597302 hey i just want to confirm the trades i have in your book trade i sell x trade i buy...\n",
            "2      4.0640     3383202.1075857656796 you fucker that s my trade i was trying to buy nines the last minutes all i got was scraps...\n",
            "3      3.9941     30793972.1075857600929 jim the list i gave you is a list of brokers that can clear through you not brokers that you...\n",
            "4      3.9600     15827855.1075857658654 torrey please set me up to trade crude john...\n",
            "5      3.9017     29790084.1075857652778 forwarded by john arnold hou ect on am zerilli frank fzerilli powermerchants com on am to jarnold enron com jarnold...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output for query = \"trade\" using Cosine similarity function:\n",
        "# 1. 0.3053     15827855.1075857658654\n",
        "# 2. 0.2994     32835197.1075857597302\n",
        "# 3. 0.2284     2752057.1075857658632\n",
        "# 4. 0.1855     5340834.1075857658345\n",
        "# 5. 0.1788     3383202.1075857656796"
      ],
      "metadata": {
        "id": "3h8H_U3q07l0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print length of terms in Doc-ID: 15827855.1075857658654\n",
        "count = 0\n",
        "for email in email_data:\n",
        "    if email[\"Document-ID\"] in [\"15827855.1075857658654\"]:\n",
        "        print(\"Total number of terms in Doc-ID 15827855.1075857658654\", len(email['content']))\n",
        "        for text in email['content']:\n",
        "          if text == 'trade':\n",
        "            count += 1\n",
        "print(\"Number of 'trade' term in Doc: \", count)\n",
        "\n",
        "# print length of terms in Doc-ID: 32835197.1075857597302\n",
        "count = 0\n",
        "for email in email_data:\n",
        "    if email[\"Document-ID\"] in [\"32835197.1075857597302\"]:\n",
        "        print(\"Total number of terms in Doc-ID 32835197.1075857597302\", len(email['content']))\n",
        "        for text in email['content']:\n",
        "          if text == 'trade':\n",
        "            count += 1\n",
        "print(\"Number of 'trade' term in Doc: \", count)\n",
        "\n",
        "count = 0\n",
        "# print length of terms in Doc-ID: 2752057.1075857658632\n",
        "for email in email_data:\n",
        "    if email[\"Document-ID\"] in [\"2752057.1075857658632\"]:\n",
        "        print(\"Total number of terms in Doc-ID 2752057.1075857658632 \", len(email['content']))\n",
        "        for text in email['content']:\n",
        "          if text == 'trade':\n",
        "            count += 1\n",
        "print(\"Number of 'trade' term in Doc: \", count)\n",
        "\n",
        "count = 0\n",
        "# print length of terms in Doc-ID: 5340834.1075857658345\n",
        "for email in email_data:\n",
        "    if email[\"Document-ID\"] in [\"5340834.1075857658345\"]:\n",
        "        print(\"Total number of terms in Doc-ID 5340834.1075857658345 \", len(email['content']))\n",
        "        for text in email['content']:\n",
        "          if text == 'trade':\n",
        "            count += 1\n",
        "print(\"Number of 'trade' term in Doc: \", count)\n",
        "\n",
        "count = 0\n",
        "# print length of terms in Doc-ID: 3383202.1075857656796\n",
        "for email in email_data:\n",
        "    if email[\"Document-ID\"] in [\"3383202.1075857656796\"]:\n",
        "        print(\"Total number of terms in Doc-ID 3383202.1075857656796\", len(email['content']))\n",
        "        for text in email['content']:\n",
        "          if text == 'trade':\n",
        "            count += 1\n",
        "print(\"Number of 'trade' term in Doc: \", count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37kb4NuWygqi",
        "outputId": "f6964487-933c-4e11-86af-98d18366424c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of terms in Doc-ID 15827855.1075857658654 9\n",
            "Number of 'trade' term in Doc:  1\n",
            "Total number of terms in Doc-ID 32835197.1075857597302 41\n",
            "Number of 'trade' term in Doc:  3\n",
            "Total number of terms in Doc-ID 2752057.1075857658632  17\n",
            "Number of 'trade' term in Doc:  1\n",
            "Total number of terms in Doc-ID 5340834.1075857658345  15\n",
            "Number of 'trade' term in Doc:  1\n",
            "Total number of terms in Doc-ID 3383202.1075857656796 58\n",
            "Number of 'trade' term in Doc:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dScZI3PvnNqV"
      },
      "source": [
        "## Observations (5 points)\n",
        "What do you observe? Are there key differences between the two ranking approaches? Briefly discuss in bullet points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gku0X4oFrBRX"
      },
      "source": [
        "* **Your observations:**\n",
        "\n",
        "Here, we can see that the relevance ordering differs when we use Cosine and BM25 methods. To better understand the difference between them let's observe these 2 Doc-IDs:\n",
        "\n",
        "1. Doc-ID 32835197.1075857597302 (**BM25 Rank 1, Cosine Rank 2**): Total terms: 41, 'trade' frequency: 3\n",
        "2. Doc-ID 15827855.1075857658654 (**BM25 Rank 4, Cosine Rank 1**): Total terms: 9, 'trade' frequency: 1\n",
        "\n",
        "We know that, BM25 prioritizes term frequency but accounts for document length normalization. For the first example(32835197.1075857597302), BM25 gives it the highest score even though the term “trade” appears only 3 times because it accounts for document length (41 words). This means BM25 likely down-weights term frequency in longer documents while still considering importance.\n",
        "\n",
        "For the second document example(15827855.1075857658654), Cosine similarity ranks this document first because \"trade\" appears 1 time in just 9 words, giving it a high TF-IDF weight. However, BM25, ranks it lower (4th place) because it accounts for term saturation—just because a term appears 1 time in a very short document doesn’t mean it's always more relevant.\n",
        "\n",
        "* **Differences:**\n",
        "\n",
        "Through this observation we notice that BM25 and Cosine Similarity are both used to measure how relevant a document is to a search query, but they work in different ways.\n",
        "\n",
        "Cosine Similarity focuses mainly on term frequency and gives higher scores to short documents where the search term appears frequently. However, this can sometimes lead to overly short documents being ranked too high, even if they are not the most relevant.\n",
        "\n",
        "On the other hand, BM25 takes document length into account and prevents long documents from being unfairly penalized. If a term appears too often in a long document, BM25 reduces its impact to avoid giving too much importance to repetitive words. At the same time, it ensures that short documents do not dominate the rankings just because they contain the term frequently. Because of this, BM25 does a better job at mimicking how real-world search engines rank documents, making it more effective for retrieving useful search results.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwQKelDxgzS8"
      },
      "source": [
        "# Part 4: Cool LLM RAG Extension (15 points)\n",
        "\n",
        "Finally, we give you an opportunity to explore using OASIS for Retrieval-Augmented Generation (RAG) with LLMs.\n",
        "Here, the task is to retrieve the top-5 emails for a query you like. You will then pass the retrieved email content along with your question to the LLM and let it answer your question. Specifically, we want you:\n",
        "\n",
        "* Query the LLM directly with your question.\n",
        "* Retrieve the top-5 emails based on your query and pass them to the LLM along with your question.\n",
        "* How is the RAG output different from the non-RAG output? Does retrieval help the LLM better answer your question?\n",
        "\n",
        "We recommend using Gemini following the [instructions](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb) here.\n",
        "\n",
        "Hint: Take a close look at the dataset and pick a specific, relevant query where retrieval can enhance the LLM’s response.\n",
        "\n",
        "*What You Will Submit*\n",
        "- Your query.\n",
        "- The top-5 retrieved emails (including Document ID and ranking score).\n",
        "- The LLM's response without retrieval.\n",
        "- The LLM's response with retrieval (RAG).\n",
        "- A brief analysis comparing both outputs.\n",
        "\n",
        "We will grade this last part according to correctness, effort, and creativity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irxiT-iTgzS9"
      },
      "source": [
        "## Step 1: Query the LLM Directly (Without Retrieval) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JiyOf67igzS9"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\" # Install the Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "WbcOKqNAu8G0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai  # Google Gemini API\n",
        "\n",
        "# Gemini API Setup\n",
        "def query_gemini(query_text):\n",
        "    \"\"\"Query Gemini LLM directly (without retrieval).\"\"\"\n",
        "    GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-pro\")\n",
        "    response = model.generate_content(query_text)\n",
        "    return response.text\n",
        "\n",
        "#  Run LLM Query\n",
        "query_text = \"Please reply\"\n",
        "\n",
        "# Step 1: Query LLM directly (Without Retrieval)\n",
        "llm_response_no_rag = query_gemini(query_text)\n",
        "\n",
        "#  Print Results\n",
        "print(\"\\n Query:\", query_text)\n",
        "print(\"\\n LLM Response Without Retrieval:\\n\")\n",
        "print(llm_response_no_rag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "kdwVKaJUwR5r",
        "outputId": "b1865b77-4cac-45e6-ba62-d11639a4bdef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Query: Please reply\n",
            "\n",
            " LLM Response Without Retrieval:\n",
            "\n",
            "Sure, I can help you with that. Please let me know what you need assistance with, and I'll do my best to provide a helpful response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otHscQFhgzTN"
      },
      "source": [
        "## Step 2: Query the LLM with Retrieved Emails (RAG) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Gemini API Setup\n",
        "def query_gemini_with_rag(query_text, retrieved_emails):\n",
        "    \"\"\"Query Gemini LLM with Retrieval-Augmented Generation (RAG).\"\"\"\n",
        "    GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-pro\")\n",
        "\n",
        "    # Prepare retrieved email context\n",
        "    email_context = \"\\n\\n\".join([f\"Email {i+1} (Doc {doc_id}): {content}\"\n",
        "                                for i, (_, doc_id, content) in enumerate(retrieved_emails)])\n",
        "\n",
        "    # Construct prompt with retrieved emails\n",
        "    prompt = f\"Based on the following emails:\\n\\n{email_context}\\n\\nQuery: {query_text}\\nAnswer:\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "#  Step 2: Retrieve Emails & Query LLM\n",
        "\n",
        "query_text = \"Please reply\"\n",
        "\n",
        "top_5_bm25_results = rank_documents_bm25(query_text, email_data)\n",
        "\n",
        "\n",
        "# Step 2.2: Query LLM with Retrieved Emails (RAG)\n",
        "llm_response_rag = query_gemini_with_rag(query_text, top_5_bm25_results)\n",
        "\n",
        "#  Print Results\n",
        "\n",
        "print(\"\\n Query:\", query_text)\n",
        "print(\"\\n Top-5 Retrieved Emails (BM25 Ranking):\")\n",
        "print(\"Rank   Score       Document-ID   Document \\n\" + \"-\" * 60)\n",
        "for rank, (score, doc_id, content) in enumerate(top_5_bm25_results, start=1):\n",
        "    print(f\"{rank:<6} {score:.4f}     {doc_id:<12} {' '.join(content[:20])}...\")  # Print first 20 words for brevity\n",
        "\n",
        "print(\"\\n LLM Response With Retrieval (RAG):\\n\")\n",
        "print(llm_response_rag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "TQhXSx9m5Yi_",
        "outputId": "9218a405-2dc1-4f6d-a55c-424558e299d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Query: Please reply\n",
            "\n",
            " Top-5 Retrieved Emails (BM25 Ranking):\n",
            "Rank   Score       Document-ID   Document \n",
            "------------------------------------------------------------\n",
            "1      7.6356     4517077.1075857600583 vlade i have not heard back from you what is the schedule for changing the var process please reply john...\n",
            "2      7.1060     24971240.1075857597324 received z zyft02 yahoo com on am please respond to zyft02 yahoo com to john arnold enron com cc subject...\n",
            "3      6.6585     32485058.1075857601648 tanya on friday i emailed a new vol curve to use for var testing i was under the impression that...\n",
            "4      6.5422     18936962.1075857656536 can you send it through lavo he s suppose to pay for it thanks john liz m taylor am to...\n",
            "5      6.2484     2564966.1075857650834 no soblander carrfut com on am to soblander carrfut com cc subject please respond carr is hosting an enymex presentation...\n",
            "\n",
            " LLM Response With Retrieval (RAG):\n",
            "\n",
            "john\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YnqxBZ4gzTN"
      },
      "source": [
        "## Discussion (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyVyTcM-gzTN"
      },
      "source": [
        "In this section, reflect on the performance of different ranking methods and the impact of retrieval on LLM responses. Consider:\n",
        "\n",
        "- How did retrieval affect the LLM’s response? Did it improve factual accuracy or relevance?\n",
        "- Were there cases where retrieval hurt performance (e.g., irrelevant documents, redundancy)?\n",
        "- Any ideas for improving the ranking or retrieval process?\n",
        "\n",
        "Keep your discussion *concise* and *insightful*, focusing on key takeaways from your experiments. Please answer in bullet points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMjckcmgzTN"
      },
      "source": [
        "*your discussion here*\n",
        "\n",
        "* ***Impact of Retrieval on LLM’s Response:***\n",
        "Retrieval helped the LLM generate more factually accurate and relevant responses by providing it with high-quality, context-specific documents.\n",
        "For example without retrieval the LLM responded giving generic answer as 'Sure, I can help you ....' but after providing it retrieved emails it gave the most relevant word to the query as 'John'\n",
        "\n",
        "* ***Cases Where Retrieval Hurt Performance:***\n",
        "\n",
        "1. If irrelevant or redundant documents are retrieved, they can mislead the LLM, introducing noise or unnecessary information into the response. For example, in this case, as humans, we understand that when we prompt the LLM with \"Please reply,\" we expect it to either ask us what to reply or provide relevant information if it knows the context. This is exactly what the LLM without retrieval did. However, when the LLM had access to our retrieved emails, it generated a response solely based on the provided information and answered 'John' which is the most relevant word to the query 'Please reply'.\n",
        "\n",
        "2. Appart from LLM part, in cosine similarity score determination, short documents with high term frequency (favored by Cosine Similarity) sometimes ranked too high, leading to responses that lacked depth.\n",
        "\n",
        "* ***Ideas for Improving Ranking & Retrieval:***\n",
        "\n",
        "1. Removing duplicate or irrelevant documents before retrieval could improve response quality.\n",
        "\n",
        "2. Instead of raw frequency weights as 1, we could assign high values to the important words. This will help in ranking and getting correct relevance order even if the term frequency of that word/term is less.\n",
        "\n",
        "3. Add stemming and lemmatization while preparing tokens. This will ensure better matching for query and documents. Moreover, it will reduce dictionary/vocabulary size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FihN3qniN-Jy"
      },
      "source": [
        "# Collaboration Declarations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKp_Fe-N-Jy"
      },
      "source": [
        "*You should fill out your collaboration declarations here.*\n",
        "\n",
        "1. Refered this stackoverflow link to write BM25 code:\n",
        "https://stackoverflow.com/questions/61877065/implementation-of-okapi-bm25-in-python\n",
        "\n",
        "2. Refered this wikipedia link to correctly implement and understand BM25 formula:\n",
        "https://en.wikipedia.org/wiki/Okapi_BM25\n",
        "\n",
        "3. Refered this link to develop cosine similarity function and tf idf function with slight variations to formula according to the lecture slides:\n",
        "https://medium.com/towards-data-science/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "\n",
        "4. Used chatgpt while remove more special characters, punctions and other things in addition to the ones given in code prompt.\n",
        "\n",
        "5. Cross checked outputs with friends and discussion forum."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}